{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca2ca94-730e-4b3b-a1e7-311c89eb787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, to_timestamp, lit, when, unix_timestamp, to_date, from_unixtime, abs, row_number\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import logging\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ddcf45-52c6-4bae-8aa5-3a242e923d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"GTFSRealtimeVPProcessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e7d19b-33b2-430b-8fd8-8e63aa432900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/21 09:56:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/21 09:56:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/21 09:56:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/21 09:56:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/08/21 09:56:28 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GTFSRealtimeMonitoring\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"America/New_York\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")  # Adjust based on cluster size\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c9f862-5b18-4b4d-9c6f-f5263d45b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_schema = StructType([\n",
    "    StructField(\"header\", StructType([\n",
    "        StructField(\"gtfsRealtimeVersion\", StringType()),\n",
    "        StructField(\"timestamp\", StringType())\n",
    "    ])),\n",
    "    StructField(\"entity\", ArrayType(StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"vehicle\", StructType([\n",
    "            StructField(\"trip\", StructType([\n",
    "                StructField(\"tripId\", StringType()),\n",
    "                StructField(\"routeId\", StringType()),\n",
    "                StructField(\"startDate\", StringType())\n",
    "            ])),\n",
    "            StructField(\"position\", StructType([\n",
    "                StructField(\"latitude\", DoubleType()),\n",
    "                StructField(\"longitude\", DoubleType())\n",
    "            ])),\n",
    "            StructField(\"timestamp\", StringType())\n",
    "        ]))\n",
    "    ])))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76840b8d-334d-4f21-90cc-67dfcf71cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_url = \"jdbc:clickhouse://clickhouse:8123\"\n",
    "clickhouse_properties = {\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"123\",  # TODO: Replace with secure credential management\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "    \"isolationLevel\": \"NONE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d53fc75-dcc8-41f0-941e-f992394fd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_clickhouse(table_name):\n",
    "    return spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"{clickhouse_url}/gtfs_batch\") \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", clickhouse_properties[\"user\"]) \\\n",
    "        .option(\"password\", clickhouse_properties[\"password\"]) \\\n",
    "        .option(\"driver\", clickhouse_properties[\"driver\"]) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c67e432-dbbf-4741-a1b8-a078399f6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "depot_to_borough = {\n",
    "    \"CS\": \"Queens\", \"QV\": \"Queens\", \"JA\": \"Queens\", \"FP\": \"Brooklyn\", \"CSg\": \"Queens\",\n",
    "    \"BP\": \"Queens\", \"LG\": \"Queens\", \"FR\": \"Queens\", \"GA\": \"Brooklyn\", \"EN\": \"Brooklyn\",\n",
    "    \"JG\": \"Brooklyn\", \"FB\": \"Brooklyn\", \"UP\": \"Brooklyn\", \"SC\": \"Brooklyn\", \"MQ\": \"Manhattan\",\n",
    "    \"QVH\": \"Manhattan\", \"MV\": \"Manhattan\", \"QU\": \"Manhattan\", \"KB\": \"Manhattan\", \"GH\": \"Bronx\",\n",
    "    \"WF\": \"Bronx\", \"EC\": \"Bronx\", \"CA\": \"Staten Island\", \"CH\": \"Staten Island\", \"MD\": \"Staten Island\",\n",
    "    \"YU\": \"Staten Island\",\n",
    "    \"JK\": \"Brooklyn\",  # For 43206923-JKPC5-JK_C5-Weekday-03\n",
    "    \"CP\": \"Brooklyn\",  # For 43120117-CPPC5-CP_C5-Weekday-03\n",
    "    \"OF\": \"Manhattan\",  # For OF_C5-Weekday-...\n",
    "    \"OH\": \"Manhattan\",  # For OH_C5-Weekday-...\n",
    "    \"YO\": \"Manhattan\"   # For 43139148-YOPC5-YO_C5-Weekday-03\n",
    "}\n",
    "mapping_expr = F.create_map([F.lit(x) for x in sum(depot_to_borough.items(), ())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c135ce-394b-4c25-87c3-e9c171f4aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = read_from_clickhouse(\"stops\").filter(col(\"is_current\") == True).select(\"stop_id\", \"stop_name\", \"stop_lat\", \"stop_lon\")\n",
    "stop_times = read_from_clickhouse(\"stop_times\").filter(col(\"is_current\") == True).select(\"trip_id\", \"arrival_time\", \"stop_id\")\n",
    "stop_boroughs = read_from_clickhouse(\"stop_boroughs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8e5f38-1108-4f87-8a54-54bb126134e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[stop_id: decimal(20,0), trip_id: string, arrival_time: string, stop_name: string, stop_lat: double, stop_lon: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_times_with_stops = stop_times.join(stops, \"stop_id\", \"inner\")\n",
    "\n",
    "\n",
    "stops.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "stop_times.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "stop_boroughs.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "stop_times_with_stops.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045918cb-504b-4e7b-bd97-3b50d510b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    if None in (lon1, lat1, lon2, lat2):\n",
    "        return None\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371000  # Radius of Earth in meters\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251c7ab2-e2b8-4674-a90d-8bed1fe47804",
   "metadata": {},
   "outputs": [],
   "source": [
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bee64a-04e4-4ac7-9155-aa2d13b01993",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_raw_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"gtfs-vehicle-positions\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df4d203-d3f5-48ac-b2d4-d84105fab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_kafka_df = vp_raw_df.selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "vp_df = vp_kafka_df.select(F.from_json(F.col(\"json_str\"), vehicle_schema).alias(\"data\")) \\\n",
    "    .select(\n",
    "        \"data.header.gtfsRealtimeVersion\",\n",
    "        F.col(\"data.header.timestamp\").cast(\"long\").cast(\"timestamp\").alias(\"header_timestamp\"),\n",
    "        F.expr(\"explode(data.entity) as entity\")\n",
    "    ) \\\n",
    "    .withColumn(\"vehicle_timestamp\", \n",
    "                F.when(F.col(\"entity.vehicle.timestamp\").cast(\"long\").isNotNull(), \n",
    "                       F.to_timestamp(F.col(\"entity.vehicle.timestamp\").cast(\"long\"))).otherwise(F.lit(None))) \\\n",
    "    .withWatermark(\"vehicle_timestamp\", \"2 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430c9bbb-e64f-40ef-b4cc-71ac73967c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_exploded_df = vp_df.select(\n",
    "    col(\"gtfsRealtimeVersion\").alias(\"gtfs_version\"),\n",
    "    col(\"header_timestamp\"),\n",
    "    col(\"entity.id\").alias(\"entity_id\"),\n",
    "    col(\"entity.vehicle.trip.tripId\").alias(\"vp_trip_id\"),\n",
    "    col(\"entity.vehicle.trip.routeId\").alias(\"route_id\"),\n",
    "    col(\"entity.vehicle.trip.startDate\").alias(\"vp_start_date\"),\n",
    "    col(\"entity.vehicle.position.latitude\").alias(\"latitude\"),\n",
    "    col(\"entity.vehicle.position.longitude\").alias(\"longitude\"),\n",
    "    col(\"vehicle_timestamp\")\n",
    ").filter(col(\"vp_trip_id\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d8c5a9f-f610-4b85-938b-11b591500c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_to_dashboard(batch_df, batch_id):\n",
    "    try:\n",
    "        logger.info(f\"Processing batch {batch_id} for dashboard\")\n",
    "        \n",
    "        # Join streaming batch with static stop_times_with_stops\n",
    "        joined_df = batch_df.join(stop_times_with_stops, batch_df[\"vp_trip_id\"] == stop_times_with_stops[\"trip_id\"], \"inner\")        \n",
    "\n",
    "        \n",
    "        # Compute scheduled_seconds from arrival_time (HH:MM:SS to seconds)\n",
    "        joined_df = joined_df.withColumn(\"scheduled_seconds\", \n",
    "                                         expr(\"cast(split(arrival_time, ':')[0] as int) * 3600 + cast(split(arrival_time, ':')[1] as int) * 60 + cast(split(arrival_time, ':')[2] as int)\"))\n",
    "        \n",
    "        # Compute scheduled_arrival timestamp\n",
    "        joined_df = joined_df.withColumn(\"scheduled_arrival\", \n",
    "                                         to_timestamp(from_unixtime(unix_timestamp(to_date(col(\"vp_start_date\"), \"yyyyMMdd\")) + col(\"scheduled_seconds\"))))\n",
    "        \n",
    "        # Filter to stops where scheduled_arrival <= vehicle_timestamp (stops the vehicle should have arrived at)\n",
    "        filtered_df = joined_df.filter((col(\"scheduled_arrival\") <= col(\"vehicle_timestamp\")) | col(\"trip_id\").isNull())\n",
    "        \n",
    "        # Window to select the latest scheduled stop (max scheduled_arrival <= vehicle_timestamp)\n",
    "        window_spec = Window.partitionBy(\"entity_id\", \"vp_trip_id\", \"vehicle_timestamp\").orderBy(col(\"scheduled_arrival\").desc())\n",
    "        selected_df = filtered_df.withColumn(\"row_num\", row_number().over(window_spec)).filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "        \n",
    "        # Compute distance to the selected stop (if any)\n",
    "        selected_df = selected_df.withColumn(\"distance\", haversine_udf(col(\"longitude\"), col(\"latitude\"), col(\"stop_lon\"), col(\"stop_lat\")))\n",
    "        \n",
    "        # Determine status and delay\n",
    "        selected_df = selected_df.withColumn(\"status\", when(col(\"distance\") <= 100, \"arrived\").otherwise(\"on_way\"))\n",
    "        selected_df = selected_df.withColumn(\"delay_seconds\", when(col(\"status\") == \"arrived\", unix_timestamp(col(\"vehicle_timestamp\")) - unix_timestamp(col(\"scheduled_arrival\"))).otherwise(lit(None)))\n",
    "        \n",
    "        # Join with stop_boroughs for borough (cast stop_id to String)\n",
    "        selected_df = selected_df.withColumn(\"depot_code\", F.regexp_extract(F.col(\"vp_trip_id\"), r\"^(?:[0-9]+-)?([A-Z]{2,3})(?:PC5-[A-Z]{2,3})?_.*\", 1)) \\\n",
    "                                 .withColumn(\"borough\", F.coalesce(mapping_expr[F.col(\"depot_code\")], F.lit(\"Unknown\"))) \\\n",
    "                                 .drop(\"depot_code\")\n",
    "        \n",
    "        selected_df = selected_df.select(\n",
    "            F.col(\"gtfs_version\"),\n",
    "            F.col(\"entity_id\"),\n",
    "            F.col(\"vp_trip_id\"),\n",
    "            F.col(\"route_id\"),\n",
    "            F.col(\"vp_start_date\"),\n",
    "            F.col(\"latitude\"),\n",
    "            F.col(\"longitude\"),\n",
    "            F.col(\"vehicle_timestamp\"),\n",
    "            F.col(\"header_timestamp\"),\n",
    "            F.col(\"stop_id\"),\n",
    "            F.col(\"stop_name\"),\n",
    "            F.col(\"stop_lat\"),\n",
    "            F.col(\"stop_lon\"),\n",
    "            F.col(\"scheduled_arrival\"),\n",
    "            F.col(\"distance\"),\n",
    "            F.col(\"status\"),\n",
    "            F.col(\"delay_seconds\"),\n",
    "            F.col(\"borough\")\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Write to dashboard table\n",
    "        selected_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"{clickhouse_url}/gtfs_dashboard\") \\\n",
    "            .option(\"dbtable\", \"trip_monitoring2\") \\\n",
    "            .option(\"user\", clickhouse_properties[\"user\"]) \\\n",
    "            .option(\"password\", clickhouse_properties[\"password\"]) \\\n",
    "            .option(\"driver\", clickhouse_properties[\"driver\"]) \\\n",
    "            .option(\"jdbcCompliant\", \"false\") \\\n",
    "            .option(\"batchsize\", 10000) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        logger.info(f\"Batch {batch_id} processed and written to dashboard\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing batch {batch_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd0605-681a-41c2-94ad-2ad6d2f1edc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 38177)\n",
      "25/08/21 09:56:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/21 09:56:36 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:GTFSRealtimeVPProcessing:Processing batch 0 for dashboard\n",
      "25/08/21 09:56:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "INFO:GTFSRealtimeVPProcessing:Batch 0 processed and written to dashboard        \n",
      "25/08/21 09:57:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 30000 milliseconds, but spent 32572 milliseconds\n"
     ]
    }
   ],
   "source": [
    "vp_query = vp_exploded_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_batch_to_dashboard) \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"check_points/trips_monitoring_checks\") \\\n",
    "    .start() \n",
    "\n",
    "vp_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f299785-33fd-47d7-841e-dfb72731f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69079a43-24d3-47a7-b501-331f2428fe40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1394f9-0736-464e-a7b4-3e32d3c35014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
